{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "해당 내용은 16년도 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) 논문의 내용의 컨셉을 구현한 것임.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "2장에서 'information compressions'을 조금 완화시키고자 context vector를 decoder의 매 step마다 넣어주었다.\n",
    "\n",
    "하지만 이것으로 source sentence의 모든 정보를 담기에는 아직 부족하다. 그래서 이번장에서는 **attention** 을 사용해서 decoding step 마다 entire source sentence를 볼 수 있게 한다.\n",
    "\n",
    "Attention은 가중치와 유사하게 0 ~ 1 사이 값을 가지고 합이 1이 되는 attention vector $a$를 구한 뒤에 hidden state $H$와 weighted sum을 하게 된다.\n",
    "\n",
    "$w = \\Sigma _i a_i h_i$\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data : 29000\n",
      "Number of valid data : 1014\n",
      "Number of test data : 1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import en_core_web_sm, de_core_news_sm\n",
    "\n",
    "spacy_de = de_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(\n",
    "tokenize=tokenize_de,\n",
    "init_token='<sos>',\n",
    "eos_token='<eos>',\n",
    "lower=True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=('.de', '.en'),\n",
    "    fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "print(f'Number of training data : {len(train_data.examples)}')\n",
    "print(f'Number of valid data : {len(valid_data.examples)}')\n",
    "print(f'Number of test data : {len(test_data.examples)}')\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "<p align=\"center\"><img src=\"../asset/3(1).png\"></p>\n",
    "\n",
    "저번의 single layer GRU와 동일하지만 이번엔 bidirectional RNN을 사용함.\n",
    "\n",
    "하나의 layer마다 두개의 RNN이 있는데 *forward RNN*은 embedded sentence를 왼쪽에서 오른쪽, *backward RNN*은 그 반대 방향으로 작동한다.\n",
    "\n",
    "`outputs`의 크기는 [src_len, batch_size, hid_dim * n_directions] 라서 세번째 axis에 forward RNN과 backward RNN의 hidden state가 포함된다.\n",
    "\n",
    "Decoder는 bidirectional 이 아니고 single context vecotr 만 필요하기 때문에 forward, backward hidden state를 concatenate 해줘서 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self ,input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        '''\n",
    "        src - [src_len, batch_size]\n",
    "        embedded - [src_len, batch_size, emb_dim]\n",
    "        outputs = [src_len, batch_size, hid_dim * num_directions]\n",
    "        hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
    "        '''\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, ], hidden[-1, :, :]), dim=1)))\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "입력으로 decoder의 previous hidden state $s_{t-1}$와 encoder에서 얻어진 forward, backward hidden state가 모두 stacked된 $H$를 받는다. \n",
    "\n",
    "출력으로는 attention vector $a_t$를 내뱉고, vector 길이는 source sentence와 같다.\n",
    "\n",
    "### Attention vector 계산\n",
    "\n",
    "1. $s_{t-1}$와 $H$ 사이의 정보를 계산한다.\n",
    "\n",
    "$E_t = tanh(attn(s_{t-1}, H))$\n",
    "\n",
    "이때 `attn`은 linear layer를 의미함\n",
    "\n",
    "위의 식을 통해 각각의 encoder hidden state와 previous hidden state가 얼마나 match 되는지 계산할 수 있다.\n",
    "\n",
    "2. 얻어진 $E_t$는 [dec_hid_dim, src_len] 인데 이를 [src_len]으로 차원 맞춰줌\n",
    "\n",
    "$\\hat{a}_t = vE_t$\n",
    "\n",
    "이때 $v$는 hidden state에서 어떤 지점(토큰)을 중요하게 볼것인지 결정해주는 weight이라고 생각할 수 있음\n",
    "\n",
    "3. 이후 attention vector 내부 값들이 0~1 사이의 값이고, 합해서 1이 되도록 softmax에 넣는다\n",
    "\n",
    "$a_t = softmax(\\hat{a}_t)$\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"../asset/3(2).png\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        hidden = [batch_size, dec_hid_dim]\n",
    "        encoder_outputs - [src_len, batch_size, enc_hid_dim * 2]\n",
    "        '''\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        # decoder hidden state를 attentino 계산을 위해 src_len 만큼 반복해서 늘려줌\n",
    "\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1) # -> [batch_size, src_len, dec_hid_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # -> [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2))) # [batch_size, src_len, dec_hid_dim]\n",
    "        attention = self.v(energy).squeeze(2) # -> [batch_size, src_len]\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Decoder는 위에서 구현한 `attention`을 가지고 있는데, 입력으로 previous hidden state $s_{t-1}$와 모든 stacked encoder hidden state $H$를 받아서 attention vector $a_t$를 생성한다.\n",
    "\n",
    "그다음 attention vector를 가중치로 활용하여 새로운 $H$를 생성한다.\n",
    "\n",
    "$w_t = a_tH$\n",
    "\n",
    "이후 Decoder RNN에 embedded input $d(y_t)$, weighted source vector $w_t$, 전 단계 hidden state $s_{t-1}$ 가 들어가서 새로운 hidden state $s_t$를 추출한다.\n",
    "\n",
    "이후 다음 단어 예측을 위해 linear layer $f$에 현 단계 hidden state와, weighted vector, embedded input이 들어간다.\n",
    "\n",
    "<p align=\"center\"><img src=\"../asset/3(3).png\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        '''\n",
    "        input - [batch_size]\n",
    "        hidden - [batch_size, dec_hid_dim]\n",
    "        encoder_outputs = [src_len, batch_size, enc_hid_dim * 2] \n",
    "        '''\n",
    "\n",
    "        input = input.unsqueeze(0) # -> [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input)) # [1, batch_size, emb_dim]\n",
    "        a = self.attention(hidden, encoder_outputs) # -> [batch_size, src_len]\n",
    "        a = a.unsqueeze(1) # -> [batch_size, 1, src_len]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # -> [batch_size, src_len, enc_hid_dim * 2]\n",
    "        \n",
    "        # `torch.bmm` : batch matrix multiplicatoin\n",
    "        weighted = torch.bmm(a, encoder_outputs) # -> [batch_size, 1, enc_hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2) # -> [1, batch_size, enc_hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2) # -> [1, batch_size, emb_dim + enc_hid_dim * 2]\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        '''\n",
    "        src - [src_len, batch_size]\n",
    "        trg - [trg_len, batch_size]\n",
    "        '''\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7854, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,518,661 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training하면서 validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01 | Time : 0m 40s | Train loss : 5.03217 | Valid loss : 4.85520\n",
      "Epoch : 02 | Time : 0m 39s | Train loss : 4.19268 | Valid loss : 4.80471\n",
      "Epoch : 03 | Time : 0m 39s | Train loss : 3.58479 | Valid loss : 3.82384\n",
      "Epoch : 04 | Time : 0m 40s | Train loss : 3.00107 | Valid loss : 3.48618\n",
      "Epoch : 05 | Time : 0m 39s | Train loss : 2.58933 | Valid loss : 3.31239\n",
      "Epoch : 06 | Time : 0m 39s | Train loss : 2.28571 | Valid loss : 3.24563\n",
      "Epoch : 07 | Time : 0m 39s | Train loss : 2.03876 | Valid loss : 3.15424\n",
      "Epoch : 08 | Time : 0m 39s | Train loss : 1.81303 | Valid loss : 3.23614\n",
      "Epoch : 09 | Time : 0m 39s | Train loss : 1.64232 | Valid loss : 3.23125\n",
      "Epoch : 10 | Time : 0m 39s | Train loss : 1.52104 | Valid loss : 3.35088\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '3-seq2seq-model.pt')\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02} | Time : {epoch_mins}m {epoch_secs}s | Train loss : {train_loss:.5f} | Valid loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.160 | Test PPL:  23.565 |\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('3-seq2seq-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch1.7.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a84903a2b3fdb7d367cfd9ea570be165ad361672f95a63c30b0740103c512e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
