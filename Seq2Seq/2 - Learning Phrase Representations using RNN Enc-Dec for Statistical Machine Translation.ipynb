{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "\n",
    "해당 내용은 2014년도 [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) 논문의 내용의 컨셉을 구현한 것임.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Encoder는 기본적으로 embedded source sequence를 통해 context vector를 만드는 역할을 한다. 이후 이 context vector를 decoder와 linear layer에 넣어 target sentence를 만들게 된다.\n",
    "\n",
    "전에 구현했던 것처럼 multi-layered LSTM를 encoder, decoder로 사용하면 정보들이 너무 압축되는데, 이를 조금 완화시키면 더 좋은 모델을 만들 수 있다.\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducible 을 위해 random seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번과 동일하게, 영어와 독일어 spacy 모델을 쓰는데, 본 논문에서는 독일어 역정렬을 수행하지 않음\n",
    "\n",
    "또한 SOS, EOS 토큰이 포함되고, 모든 문자가 소문자가 되도록 Field를 생성해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm, de_core_news_sm\n",
    "\n",
    "spacy_de = de_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(\n",
    "tokenize=tokenize_de,\n",
    "init_token='<sos>',\n",
    "eos_token='<eos>',\n",
    "lower=True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    lower=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번과 동일한 `Multi30k dataset` 불러오고 단어들을 저장할 vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data : 29000\n",
      "Number of valid data : 1014\n",
      "Number of test data : 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=('.de', '.en'),\n",
    "    fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "print(f'Number of training data : {len(train_data.examples)}')\n",
    "print(f'Number of valid data : {len(valid_data.examples)}')\n",
    "print(f'Number of test data : {len(test_data.examples)}')\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`device`에서 연산되는 데이터 로더를 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "저번의 multi-layer LSTM을 사용했던 것과 달리 single-layer GRU를 만들어줌.\n",
    "\n",
    "Dropout은 layer와 layer 사이에서 적용되기 때문에 `dropout`은 사용하지 않음.\n",
    "\n",
    "GRU는 LSTM과 다르게 cell state 가 없고 오직 hidden state만 수행한다.\n",
    "\n",
    "$h_t = GRU(e(x_t), h_{t-1})$\n",
    "\n",
    "$(h_t, c_t) = LSTM(e(x_t), h_{t-1}, c_{t-1})$\n",
    "\n",
    "$h_t = RNN(e(x_t), h_{t-1})$\n",
    "\n",
    "위의 식을 보면 GRU와 RNN의 차이가 없어보일 수도 있지만, *gating mechanism*이 다르다.\n",
    "\n",
    "LSTM : forget gate, input gate, output gate + cell state, hidden state\n",
    "\n",
    "GRU : reset gate, update gate + hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        '''\n",
    "        src - [src_len, batch_size] \n",
    "\n",
    "        embedded - [src_len, batch_size, emb_dim]\n",
    "\n",
    "        outputs - [src_len, batch_size, hid_dim * n_directions]\n",
    "\n",
    "        hidden - [n_layers * n_directions, batch_size, hid_dim]\n",
    "        '''\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<p align=\"center\"><img src=\"../asset/2(1).png\"></p>\n",
    "\n",
    "입력으로 embedded target token $d(y_t)$ 와 전 단계의 hidden state $s_{t-1}$, 그리고 context vector $z$를 받는다.\n",
    "\n",
    "이때 context vector는 아래첨자 $t$가 없는데, 모든 decoding 단계에서 동일한 vector를 사용하기 때문임\n",
    "\n",
    "참고로 initial hidden state $s_0$는 context vecotr $z$ 이다.\n",
    "\n",
    "이처럼 RNN에서 전단계의 hidden state만 활용한 것과 다르게, context vector가 항상 주어지기 때문에 정보손실이 덜 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        '''\n",
    "        input - [batch_size]\n",
    "        hidden - [n_layers * n_directions, batch_size, hid_dim]\n",
    "        context - [same as hidden]\n",
    "\n",
    "        이때 n_layers와 n_direction은 1이다.\n",
    "\n",
    "        embedded - [1, batch_size, emb_dim]\n",
    "        emb_con = [1, batch_size, emb_dim + hid_dim]\n",
    "\n",
    "        output = [seq_len, batch_size, hid_dim * n_directions]\n",
    "        hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        '''\n",
    "        input = input.unsqueeze(0) # [batch_size] -> [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "\n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "        prediction = self.fc_out(output)\n",
    "\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model\n",
    "\n",
    "최종 모델은 아래 그림과 같이 구성된다.\n",
    "\n",
    "<p align=\"center\"><img src=\"../asset/2(2).png\"></p>\n",
    "\n",
    "디코딩은 아래와 같은 과정을 통해 순차적으로 진행된다.\n",
    "\n",
    "- 입력 토큰 $y$와 전 단계의 hidden state $s_{t-1}$, context vector $z$가 decoder에 들어감\n",
    "\n",
    "- 예측값 $\\hat{y}_{t+1}$과 새로운 hidden state $s_t$를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, 'Hidden dimension of enc and dec should be the same'\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        context = self.encoder(src)\n",
    "\n",
    "        hidden = context # Initial hidden state는 context vector 이다.\n",
    "\n",
    "        input = trg[0,:] # 디코더에는 우선 <sos> 토큰을 넣는다.\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            if teacher_force:\n",
    "                input = trg[t]\n",
    "            else:\n",
    "                input = top1\n",
    "\n",
    "        return outputs               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7854, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,220,037 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation 일때는 teacher_force를 꺼야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01 | Time : 0m 19s | Train loss : 5.04071 | Valid loss : 5.08895\n",
      "Epoch : 02 | Time : 0m 19s | Train loss : 4.39584 | Valid loss : 5.15434\n",
      "Epoch : 03 | Time : 0m 19s | Train loss : 4.07433 | Valid loss : 4.68725\n",
      "Epoch : 04 | Time : 0m 19s | Train loss : 3.75137 | Valid loss : 4.42824\n",
      "Epoch : 05 | Time : 0m 19s | Train loss : 3.45564 | Valid loss : 4.15878\n",
      "Epoch : 06 | Time : 0m 19s | Train loss : 3.15959 | Valid loss : 3.98140\n",
      "Epoch : 07 | Time : 0m 18s | Train loss : 2.90866 | Valid loss : 3.89681\n",
      "Epoch : 08 | Time : 0m 19s | Train loss : 2.68264 | Valid loss : 3.71527\n",
      "Epoch : 09 | Time : 0m 19s | Train loss : 2.49487 | Valid loss : 3.68103\n",
      "Epoch : 10 | Time : 0m 19s | Train loss : 2.30219 | Valid loss : 3.66900\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1 # weight clipping을 위함\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '2-seq2seq-model.pt')\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02} | Time : {epoch_mins}m {epoch_secs}s | Train loss : {train_loss:.5f} | Valid loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.583 | Test PPL:  35.986 |\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('2-seq2seq-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch1.7.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a84903a2b3fdb7d367cfd9ea570be165ad361672f95a63c30b0740103c512e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
